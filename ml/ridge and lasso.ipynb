{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 12})\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "train = pd.read_csv('dummies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.574302</td>\n",
       "      <td>-0.457545</td>\n",
       "      <td>-1.426423</td>\n",
       "      <td>-0.496111</td>\n",
       "      <td>-0.954345</td>\n",
       "      <td>-0.752722</td>\n",
       "      <td>0.182600</td>\n",
       "      <td>-0.516395</td>\n",
       "      <td>-0.694392</td>\n",
       "      <td>-0.670938</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.15974</td>\n",
       "      <td>-0.104812</td>\n",
       "      <td>5.291071</td>\n",
       "      <td>-0.214775</td>\n",
       "      <td>-0.202444</td>\n",
       "      <td>-0.177522</td>\n",
       "      <td>-0.08257</td>\n",
       "      <td>-0.066551</td>\n",
       "      <td>-0.15974</td>\n",
       "      <td>-0.104812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.133909</td>\n",
       "      <td>-0.471058</td>\n",
       "      <td>0.112116</td>\n",
       "      <td>-0.041359</td>\n",
       "      <td>0.136217</td>\n",
       "      <td>0.558118</td>\n",
       "      <td>0.788216</td>\n",
       "      <td>-0.524993</td>\n",
       "      <td>-0.476987</td>\n",
       "      <td>-0.341778</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.15974</td>\n",
       "      <td>-0.104812</td>\n",
       "      <td>-0.188998</td>\n",
       "      <td>-0.214775</td>\n",
       "      <td>-0.202444</td>\n",
       "      <td>-0.177522</td>\n",
       "      <td>-0.08257</td>\n",
       "      <td>-0.066551</td>\n",
       "      <td>-0.15974</td>\n",
       "      <td>-0.104812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.744681</td>\n",
       "      <td>-0.381384</td>\n",
       "      <td>0.562060</td>\n",
       "      <td>0.034687</td>\n",
       "      <td>0.230282</td>\n",
       "      <td>0.844372</td>\n",
       "      <td>0.090073</td>\n",
       "      <td>-0.392332</td>\n",
       "      <td>-0.333278</td>\n",
       "      <td>-0.243521</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.15974</td>\n",
       "      <td>-0.104812</td>\n",
       "      <td>-0.188998</td>\n",
       "      <td>-0.214775</td>\n",
       "      <td>-0.202444</td>\n",
       "      <td>-0.177522</td>\n",
       "      <td>-0.08257</td>\n",
       "      <td>-0.066551</td>\n",
       "      <td>-0.15974</td>\n",
       "      <td>-0.104812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.493665</td>\n",
       "      <td>1.089026</td>\n",
       "      <td>-1.049039</td>\n",
       "      <td>-1.027572</td>\n",
       "      <td>-1.317950</td>\n",
       "      <td>0.036797</td>\n",
       "      <td>-1.433576</td>\n",
       "      <td>0.888829</td>\n",
       "      <td>0.837269</td>\n",
       "      <td>1.075578</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.15974</td>\n",
       "      <td>-0.104812</td>\n",
       "      <td>-0.188998</td>\n",
       "      <td>-0.214775</td>\n",
       "      <td>-0.202444</td>\n",
       "      <td>-0.177522</td>\n",
       "      <td>-0.08257</td>\n",
       "      <td>-0.066551</td>\n",
       "      <td>-0.15974</td>\n",
       "      <td>-0.104812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.077808</td>\n",
       "      <td>-0.974707</td>\n",
       "      <td>0.032275</td>\n",
       "      <td>0.105820</td>\n",
       "      <td>0.001989</td>\n",
       "      <td>0.858617</td>\n",
       "      <td>0.781744</td>\n",
       "      <td>-0.941401</td>\n",
       "      <td>-0.915482</td>\n",
       "      <td>-1.066422</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.15974</td>\n",
       "      <td>-0.104812</td>\n",
       "      <td>-0.188998</td>\n",
       "      <td>-0.214775</td>\n",
       "      <td>-0.202444</td>\n",
       "      <td>-0.177522</td>\n",
       "      <td>-0.08257</td>\n",
       "      <td>-0.066551</td>\n",
       "      <td>-0.15974</td>\n",
       "      <td>-0.104812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.574302 -0.457545 -1.426423 -0.496111 -0.954345 -0.752722  0.182600   \n",
       "1  0.133909 -0.471058  0.112116 -0.041359  0.136217  0.558118  0.788216   \n",
       "2 -0.744681 -0.381384  0.562060  0.034687  0.230282  0.844372  0.090073   \n",
       "3 -0.493665  1.089026 -1.049039 -1.027572 -1.317950  0.036797 -1.433576   \n",
       "4 -0.077808 -0.974707  0.032275  0.105820  0.001989  0.858617  0.781744   \n",
       "\n",
       "        7         8         9      ...         91        92        93   \\\n",
       "0 -0.516395 -0.694392 -0.670938    ...    -0.15974 -0.104812  5.291071   \n",
       "1 -0.524993 -0.476987 -0.341778    ...    -0.15974 -0.104812 -0.188998   \n",
       "2 -0.392332 -0.333278 -0.243521    ...    -0.15974 -0.104812 -0.188998   \n",
       "3  0.888829  0.837269  1.075578    ...    -0.15974 -0.104812 -0.188998   \n",
       "4 -0.941401 -0.915482 -1.066422    ...    -0.15974 -0.104812 -0.188998   \n",
       "\n",
       "        94        95        96       97        98       99        100  \n",
       "0 -0.214775 -0.202444 -0.177522 -0.08257 -0.066551 -0.15974 -0.104812  \n",
       "1 -0.214775 -0.202444 -0.177522 -0.08257 -0.066551 -0.15974 -0.104812  \n",
       "2 -0.214775 -0.202444 -0.177522 -0.08257 -0.066551 -0.15974 -0.104812  \n",
       "3 -0.214775 -0.202444 -0.177522 -0.08257 -0.066551 -0.15974 -0.104812  \n",
       "4 -0.214775 -0.202444 -0.177522 -0.08257 -0.066551 -0.15974 -0.104812  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train)\n",
    "scaled = scaler.transform(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaled_features = StandardScaler().fit_transform(train.values)\n",
    "scaled_features_df = pd.DataFrame(scaled_features, index=train.index, columns=train.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting input variables from target\n",
    "X = train.drop('target', axis=1)\n",
    "y = train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaled_features = StandardScaler().fit_transform(X.values)\n",
    "scaled_features_df = pd.DataFrame(scaled_features, index=train.index, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_features_df,y,test_size=.1, random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MAE is 0.9461\n"
     ]
    }
   ],
   "source": [
    "mean_train = np.mean(y_train)\n",
    "# Get predictions on the test set (np.ones gives column of all 1s in given shape)\n",
    "baseline_predictions = np.ones(y_test.shape) * mean_train\n",
    "# Compute MAE\n",
    "mae_baseline = mean_absolute_error(y_test, baseline_predictions)\n",
    "print(\"Baseline MAE is {:.4f}\".format(mae_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "predictions = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear regression MAE 0.9789483429148828\n"
     ]
    }
   ],
   "source": [
    "print( 'linear regression MAE', mean_absolute_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha 1 0.9783236211938967\n",
      "alpha 101 0.9694642166306435\n",
      "alpha 201 0.9654668076301242\n",
      "alpha 301 0.9629861368534537\n",
      "alpha 401 0.9611679830808629\n",
      "alpha 501 0.9597285195187386\n",
      "alpha 601 0.9585552122200505\n",
      "alpha 701 0.9575683050196192\n",
      "alpha 801 0.9567268821384342\n",
      "alpha 901 0.9559939064346155\n",
      "alpha 1001 0.9553344642670891\n",
      "alpha 1101 0.9547348468872124\n",
      "alpha 1201 0.9541848693244407\n",
      "alpha 1301 0.9536784725712077\n",
      "alpha 1401 0.9532142525052695\n",
      "alpha 1501 0.9527804574086438\n",
      "alpha 1601 0.9523795934671198\n",
      "alpha 1701 0.9520074050729599\n",
      "alpha 1801 0.9516549560284732\n",
      "alpha 1901 0.9513248949576247\n",
      "alpha 2001 0.95102717311525\n",
      "alpha 2101 0.9507466683628097\n",
      "alpha 2201 0.9504840232239129\n",
      "alpha 2301 0.9502368378171078\n",
      "alpha 2401 0.9499996581739829\n",
      "alpha 2501 0.9497717407984833\n",
      "alpha 2601 0.9495524264162983\n",
      "alpha 2701 0.9493411272713506\n",
      "alpha 2801 0.9491442944364502\n",
      "alpha 2901 0.9489549751918537\n",
      "alpha 3001 0.9487719743325043\n",
      "alpha 3101 0.9485949165384592\n",
      "alpha 3201 0.9484234603150145\n",
      "alpha 3301 0.9482572938844008\n",
      "alpha 3401 0.9480961316964109\n",
      "alpha 3501 0.9479408830417534\n",
      "alpha 3601 0.947804123317563\n",
      "alpha 3701 0.9476737730734739\n",
      "alpha 3801 0.9475475577034929\n",
      "alpha 3901 0.947425741291599\n",
      "alpha 4001 0.9473072597993135\n",
      "alpha 4101 0.9471919607279483\n",
      "alpha 4201 0.9470797018180747\n",
      "alpha 4301 0.9469703501269177\n",
      "alpha 4401 0.9468658446643209\n",
      "alpha 4501 0.946764668888802\n",
      "alpha 4601 0.9466659975834125\n",
      "alpha 4701 0.94657652531788\n",
      "alpha 4801 0.9464898110885338\n",
      "alpha 4901 0.9464051519645915\n",
      "alpha 5001 0.9463262941702539\n",
      "alpha 5101 0.9462517649861374\n",
      "alpha 5201 0.9461789724689144\n",
      "alpha 5301 0.9461078517093982\n",
      "alpha 5401 0.946038341253696\n",
      "alpha 5501 0.9459705512943938\n",
      "alpha 5601 0.9459078494849825\n",
      "alpha 5701 0.9458465239370781\n",
      "alpha 5801 0.9457865265928878\n",
      "alpha 5901 0.9457278117676066\n",
      "alpha 6001 0.9456703359955028\n",
      "alpha 6101 0.9456152908963197\n",
      "alpha 6201 0.9455614720682367\n",
      "alpha 6301 0.9455087545110555\n",
      "alpha 6401 0.9454571028639219\n",
      "alpha 6501 0.945406483358936\n",
      "alpha 6601 0.9453568637273131\n",
      "alpha 6701 0.9453082131124252\n",
      "alpha 6801 0.9452605019891082\n",
      "alpha 6901 0.9452137020887039\n",
      "alpha 7001 0.9451677863293411\n",
      "alpha 7101 0.9451227287510231\n",
      "alpha 7201 0.9450785044551211\n",
      "alpha 7301 0.9450350895479168\n",
      "alpha 7401 0.9449924610878708\n",
      "alpha 7501 0.9449517682436785\n",
      "alpha 7601 0.944912222365389\n",
      "alpha 7701 0.9448733768221962\n",
      "alpha 7801 0.9448352125090377\n",
      "alpha 7901 0.9447977110408106\n",
      "alpha 8001 0.9447608547172921\n",
      "alpha 8101 0.9447246264901681\n",
      "alpha 8201 0.9446890099320218\n",
      "alpha 8301 0.9446548681588656\n",
      "alpha 8401 0.9446222030018308\n",
      "alpha 8501 0.9445900756790571\n",
      "alpha 8601 0.9445584725798779\n",
      "alpha 8701 0.9445273805670903\n",
      "alpha 8801 0.9444978291408521\n",
      "alpha 8901 0.9444702150120201\n",
      "alpha 9001 0.9444471318510782\n",
      "alpha 9101 0.9444250609652073\n",
      "alpha 9201 0.944403365169747\n",
      "alpha 9301 0.9443820352402896\n",
      "alpha 9401 0.94436106225859\n",
      "alpha 9501 0.9443410489307188\n",
      "alpha 9601 0.9443215604035514\n",
      "alpha 9701 0.944302392915293\n",
      "alpha 9801 0.9442846212762077\n",
      "alpha 9901 0.944267885883284\n",
      "alpha 10001 0.9442514202771073\n",
      "alpha 10101 0.9442352181565656\n",
      "alpha 10201 0.944219273418197\n",
      "alpha 10301 0.9442035801483122\n",
      "alpha 10401 0.944188132615496\n",
      "alpha 10501 0.9441729252634731\n",
      "alpha 10601 0.9441579527043134\n",
      "alpha 10701 0.9441432097119581\n",
      "alpha 10801 0.9441286912160483\n",
      "alpha 10901 0.9441143922960398\n",
      "alpha 11001 0.944100308175591\n",
      "alpha 11101 0.9440864342172043\n",
      "alpha 11201 0.9440733872015863\n",
      "alpha 11301 0.9440606432503046\n",
      "alpha 11401 0.9440480855855344\n",
      "alpha 11501 0.9440357102728449\n",
      "alpha 11601 0.9440235134886819\n",
      "alpha 11701 0.9440114915164161\n",
      "alpha 11801 0.9439996407425618\n",
      "alpha 11901 0.9439879576531575\n",
      "alpha 12001 0.9439764388303024\n",
      "alpha 12101 0.943965080948836\n",
      "alpha 12201 0.9439538807731613\n",
      "alpha 12301 0.9439441828232146\n",
      "alpha 12401 0.9439351610168548\n",
      "alpha 12501 0.9439262692435526\n",
      "alpha 12601 0.9439175048942571\n",
      "alpha 12701 0.9439088654287936\n",
      "alpha 12801 0.9439003483735791\n",
      "alpha 12901 0.943891951319427\n",
      "alpha 13001 0.9438836719194452\n",
      "alpha 13101 0.94387550788701\n",
      "alpha 13201 0.9438674569938242\n",
      "alpha 13301 0.9438595170680506\n",
      "alpha 13401 0.943851685992515\n",
      "alpha 13501 0.9438439617029816\n",
      "alpha 13601 0.9438363421864913\n",
      "alpha 13701 0.9438288254797652\n",
      "alpha 13801 0.9438214096676677\n",
      "alpha 13901 0.9438140928817245\n",
      "alpha 14001 0.9438068732986997\n",
      "alpha 14101 0.9437997491392238\n",
      "alpha 14201 0.9437927186664692\n",
      "alpha 14301 0.9437857801848784\n",
      "alpha 14401 0.9437789320389349\n",
      "alpha 14501 0.9437721726119803\n",
      "alpha 14601 0.9437655003250722\n",
      "alpha 14701 0.9437589136358824\n",
      "alpha 14801 0.943752411037636\n",
      "alpha 14901 0.9437459910580854\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 15000, 100):\n",
    "    rr = Ridge(alpha=i)\n",
    "    rr.fit(X_train, y_train)\n",
    "    predictions = rr.predict(X_test)\n",
    "    print('alpha', i, mean_absolute_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ridge alpha value increasing improves MAE up until about 100000. This basically means the model does not fit, ignoring the indiviual values and fitting the mean.\n",
    "\n",
    "### lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha 1 0.944251583618581\n",
      "alpha 10 0.944251583618581\n",
      "alpha 100 0.944251583618581\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "for i in [1,10,100]:\n",
    "    las = linear_model.Lasso(alpha=i)\n",
    "    las.fit(X_train, y_train)\n",
    "    predictions = rr.predict(X_test) \n",
    "    print('alpha', i, mean_absolute_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha 0.05 0.944251583618581\n",
      "alpha 1 0.944251583618581\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "for i in [.05, 1]:\n",
    "    las = linear_model.Lasso(alpha=i)\n",
    "    las.fit(X_train, y_train)\n",
    "    predictions = rr.predict(X_test) \n",
    "    print('alpha', i, mean_absolute_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
